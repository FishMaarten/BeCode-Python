{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing in nature compares to the complex information processing and pattern recognition abilities of our brains. In our quest to advance technology, we are now developing algorithms that mimic the network of our brains━these are called deep neural networks.\n",
    "\n",
    "Deep neural networks have a unique structure because they have a relatively large and complex hidden component between the input and output layers. To be considered a deep neural network, this hidden component must contain at least two layers.\n",
    "\n",
    "![](assets/dnn_structure.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weight is assigned to each connection from one node to another, signifying the strength of the connection between the two nodes. A weighted sum of all the connections to a specific node is computed and converted to a number between zero and one by an activation function. The result is then passed on to the next node in the network.\n",
    "\n",
    "This process continues until the output nodes are reached. The output nodes are categories, such as cats, zebras or cars. As the model learns, the weights between the connection are continuously updated.\n",
    "\n",
    "Because of their structure, deep neural networks have a greater ability to recognize patterns than shallow networks. Deep neural networks classify data based on certain inputs after being trained with labeled data. Meaning, they can learn by being exposed to examples without having to be programmed with explicit rules for every task.\n",
    "\n",
    "For example, if we want to build a model that will identify cat pictures, we can train the model by exposing it to labeled pictures of cats. Over time, the model will learn to identify the generic features of cats, such as pointy ears, the general shape, and tail, and it will be able to identify an unlabeled cat picture it has never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's understand with an example\n",
    "\n",
    "Imagine you work for a loan company, and you need to build a model for predicting, whether a user (borrower) should get a loan or not? You have the features for each customer like age, bank balance, salary per annum, whether retired or not and so on.\n",
    "\n",
    "![](assets/ex1.jpg)\n",
    "\n",
    "Consider if you want to solve this problem using a linear regression model, then the linear regression will assume that the outcome (whether a customer's loan should be sanctioned or not) will be the sum of all the features. It will take into account the effect of age, salary, bank balance, retirement status and so. So the linear regression model is not taking into account the interaction between these features or how they affect the overall loan process.\n",
    "\n",
    "![](assets/ex2.jpg)\n",
    "\n",
    "The above figure left **(A)** shows prediction from a linear regression model with absolutely no interactions in which it simply adds up the effect of age (30 > age > 30) and bank balance, you can observe from figure (A) that the lack of interaction is reflected by both lines being parallel that is what the linear regression model assumes.\n",
    "\n",
    "On the other hand, figure right **(B)** shows predictions from a model that allows interactions in which the lines do not have to parallel. Neural Networks is a pretty good modeling approach that allows interactions like the one in figure (B) very well and from these neural networks evolves a term known as Deep Learning which uses these powerful neural networks. Because the neural network takes into account these type of interactions so well it can perform quite well on a plethora of prediction problems you have seen till now or possibly not heard.\n",
    "\n",
    "Since neural networks are capable of handling such complex interactions gives them the power to solve challenging problems and do amazing things with\n",
    "\n",
    "    1. Image\n",
    "    2. Text\n",
    "    3. Audio\n",
    "    4. Video\n",
    "    \n",
    "This list is merely a subset of what neural networks are capable of solving, almost anything you can think of in data science field can be solved with neural networks.\n",
    "\n",
    "Deep learning can even learn to write a code for you. Well isn't that super amazing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions in Neural Network\n",
    "\n",
    "\n",
    "![](assets/ex3.jpg)\n",
    "\n",
    "The neural network architecture looks something similar to the above figure. On the far left you have the **input layer** that consists of the features like age, salary per annum, bank balance, etc. and on the far right, you have the **output layer** that outputs the prediction from the model which in your case is whether a customer should get a loan or not.\n",
    "\n",
    "The layers apart from the input and the output layers are called the **hidden layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why they are called hidden layers?\n",
    "\n",
    "Well, one good reason is while the input and output layers correspond to apparent things that occur or are present in the world and can be stored as data but the values in the hidden layers are not something that relates to the real world or something for which have data.\n",
    "\n",
    "Technically, each node in the hidden layer represents an aggregation of information from the input data; hence each node adds to the model's capability to capture interactions between the data. The more the nodes, the more interactions can be achieved from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "Let's start by seeing how neural networks use data to make predictions which is taken care by the forward propagation algorithm.\n",
    "\n",
    "To understand the concept of forward propagation let's revisit the example of a loan company. For simplification, let's consider only two features as an input namely age and retirement status, the retirement status being a binary ( 0 - not retired and 1 - retired) number based on which you will make predictions.\n",
    "\n",
    "![](assets/forward_propagation.jpg)\n",
    "\n",
    "The above figure shows a customer with age 40 and is not retired. The forward propagation algorithm will pass this information through the network/model to predict the output layer. The lines connect each node of the input to every other node of the hidden layer. Each line has a weight associated with it which indicates how strongly that feature affects the hidden node connected to that specific line.\n",
    "\n",
    "There are total four weights between input and hidden layer. The first set of weights are connected from the top node of the input layer to the first and second node of the hidden layer; likewise, the second set of weight are connected from the bottom node of the input to the first and second node of the hidden layer.\n",
    "\n",
    "Remember these weights are the key in deep learning which you train or update when you fit a neural network to the data. These weights are commonly known as **parameters*.\n",
    "\n",
    "To make a prediction for the top node of the hidden layer, you consider each node in the input layer multiply it by the weights connected to that top node and finally sum up all the values resulting in a value 40 (40 * 1 + 0 * 1 = 40) as shown in above figure. You repeat the same process for the bottom node of the hidden layer resulting in a value 40. Finally, for the output layer you follow the same process and obtain a value 0 (40 * 1 + 40 * (-1) = 0). This output layer predicts a value zero.\n",
    "\n",
    "Now you might wonder what the relevance of value zero is, well you consider the loan problem as binary classification in which an output of zero indicates a loan sanction and an output of one indicates a loan prohibition.\n",
    "\n",
    "That's pretty much what happens in forward propagation. You start from the input layer move to the hidden layer and then to the output layer which then gives you a prediction score. You pretty much always use the multiple-add process, in linear algebra this operation is a dot product operation. In general, a forward propagation is done for a single data point at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation algorithm coding example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will create a numpy array of input_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([40,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the input data, now you will create a dictionary called weights in which the keys of the dictionary will hold the variable names for node0 and node1 of hidden layers and an output node for the output layer. The values of the dictionary will be the parameters (weight values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'node0':([1,1]),\n",
    "          'node1':([1,-1]),\n",
    "          'output':([1,-1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly calculate the value of node0 of the hidden layer. You first multiply the input_data with the weights of node0 and then use a sum() function to obtain a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0_value = (input_data * weights['node0']).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will do the same for the node1 of the hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1_value = (input_data * weights['node1']).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let's create a numpy array of the hidden layer values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 40])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_values = np.array([node0_value,node1_value])\n",
    "\n",
    "hidden_layer_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you multiply the hidden layer values with the weights of the output layer and again use a sum() function to obtain a prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (hidden_layer_values * weights['output']).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the output and see if it matches the output you should expect!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiply-add process is only half part of how a neural network works; there's more to it!\n",
    "\n",
    "To utilize the maximum predictive power, a neural network uses an activation function in the hidden layers. An activation function allows the neural network to capture non-linearities present in the data.\n",
    "\n",
    "![](assets/activation_functions.jpg)\n",
    "\n",
    "In neural networks, often time the data that you work with is not linearly separable and to find a decision boundary that can separate the data points you need some non-linearity in your network. For example, A customer has no previous loan record compared to a customer having a previous loan record may impact the overall output differently.\n",
    "\n",
    "If the relationships in the data aren't straight or linear, then you need a non-linear activation function to capture the non-linearity. An activation function is applied to the value coming into a node which then transforms it into the value stored in that node or the node output.\n",
    "\n",
    "Let's apply an **s-shaped activation function** called tanh to the nodes of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0_act = np.tanh(node0_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1_act = np.tanh(node1_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_values_act = np.array([node0_act,node1_act])\n",
    "hidden_layer_values_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the difference in the hidden_layer_values and hidden_layer_values_act.\n",
    "\n",
    "Let's quickly calculate the output using the hidden_layer_values_act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (hidden_layer_values_act * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's time, an activation function called Rectifier Linear Unit (ReLU) is widely used in both industry and research. Even though it has two linear pieces, it's very powerful when combined through multiple hidden layers. ReLU is half rectified from the bottom as shown in the figure below.\n",
    "\n",
    "![](assets/relu.jpg)\n",
    "\n",
    "Now you will apply ReLU as the activation function on the hidden layer nodes and calculate the network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "\n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Output: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(\"Model's Output:\",model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significant difference between traditional neural networks and the modern deep learning that makes use of neural networks is the use of not just one but many successive hidden layers. Research shows that increasing the number of hidden layers massively improves the performance making the network capable of more and more interactions.\n",
    "\n",
    "The working in a network with just a single hidden layer and with multiple hidden layers remain the same. You forward propagate through these successive hidden layers as you did in the previous example with one hidden layer.\n",
    "\n",
    "![](assets/deeper_networks.jpg)\n",
    "\n",
    "Let's understand some essential facts about these deep networks!\n",
    "\n",
    "   1. Deep Learning networks are capable of internally building up representations of the pattern in the data that are essential for making accurate predictions;\n",
    "   \n",
    "   2. The patterns in the initial layers are simple, but as you go through successive hidden layers or deep into the network the network starts learning more and more complex patterns;\n",
    "   \n",
    "   3. Deep learning networks eliminate the need for handcrafted features. You do not need to create better predictive features which you then feed to the deep learning network, the network itself learns meaningful features from the data and using which it makes predictions;\n",
    "   \n",
    "   4. Deep learning is also called Representation Learning since the subsequent layers in the network build increasingly sophisticated representations of the data until you reach to the final layer where it finally makes the prediction.\n",
    "    \n",
    "\n",
    "The input to the above network is images of humans; You can see that the initial layers in the network are capturing the patterns of local contrast that are conceptually simple, patterns like vertical edges, horizontal, diagonal edges, blurry areas, etc. Once the network identifies where are these diagonal or horizontal lines the successive layers then combine that information to find larger patterns like eyes, nose, lips, etc. A much later layer might combine these patterns to find much larger abstract patterns like for example a face as depicted in the above figure.\n",
    "\n",
    "Well, the cool thing about deep learning is you don't explicitly tell the network to look for diagonal lines or wherein the image is the nose or a lip, instead of when you train the network the neural network has weights that are learned to find the relevant patterns to make accurate predictions. The learning process in neural networks off course is a gradual process in which the network undergoes multiple pieces of training before it can learn to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation in a multi-layer neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'node0_0':([2,4]),\n",
    "          'node0_1':([4,-5]),\n",
    "          'node1_0':([-1,2]),\n",
    "          'node1_1':([1,2]),\n",
    "          'output':([2,7])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/multi_layer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer 1 Output: [26  0]\n",
      "Hidden Layer 2 Output: [ 0 26]\n",
      "Model's Prediction: 182\n"
     ]
    }
   ],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data* weights['node0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data* weights['node0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    print(\"Hidden Layer 1 Output:\", hidden_0_outputs)\n",
    "\n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input =  (hidden_0_outputs* weights['node1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs* weights['node1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "    print(\"Hidden Layer 2 Output:\", hidden_1_outputs)\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(\"Model's Prediction:\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "- http://neuralnetworksanddeeplearning.com/index.html\n",
    "- https://www.investopedia.com/terms/n/neuralnetwork.asp\n",
    "- https://www.kdnuggets.com/2020/02/deep-neural-networks.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
