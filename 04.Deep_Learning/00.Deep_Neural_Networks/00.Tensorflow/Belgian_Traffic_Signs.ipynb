{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve got TensorFlow installed and imported into your workspace and you’ve gone through the basics of working with this package, it’s time to leave this aside for a moment and turn your attention to your data. Just like always, you’ll first take your time to explore and understand your data better before you start modeling your neural network.\n",
    "\n",
    "Even though traffic is a topic that is generally known amongst you all, it doesn’t hurt going briefly over the observations that are included in this dataset to see if you understand everything before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some insights\n",
    "\n",
    " - Belgian traffic signs are usually in Dutch and French. This is good to know, but for the dataset that you’ll be working with, it’s not too important!\n",
    " \n",
    " - There are six categories of traffic signs in Belgium: warning signs, priority signs, prohibitory signs, mandatory signs, signs related to parking and standing still on the road and, lastly, designatory signs.\n",
    " \n",
    " - On January 1st, 2017, more than 30,000 traffic signs were removed from Belgian roads. These were all prohibitory signs relating to speed.\n",
    " \n",
    " - Talking about removal, the overwhelming presence of traffic signs has been an ongoing discussion in Belgium (and by extension, the entire European Union)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading And Exploring The Data\n",
    "\n",
    "Now that you have gathered some more background information, it’s time to download the dataset [here](https://btsd.ethz.ch/shareddata/). You should get the two zip files listed next to \"BelgiumTS for Classification (cropped images), which are called \"BelgiumTSC_Training\" and \"BelgiumTSC_Testing\".\n",
    "\n",
    "Tip: if you have downloaded the files take a look at the folder structure of the data that you’ve downloaded! You’ll see that the testing, as well as the training data folders, contain 61 subfolders, which are the 62 types of traffic signs that you’ll use for classification in this tutorial. Additionally, you’ll find that the files have the file extension .ppm or Portable Pixmap Format. You have downloaded images of the traffic signs!\n",
    "\n",
    "Let’s get started with importing the data into your workspace. Let’s start with the lines of code that appear below the User-Defined Function (UDF) load_data():\n",
    "\n",
    "1. First, set your ROOT_PATH. This path is the one where you have made the directory with your training and test data.\n",
    " \n",
    " \n",
    "2. Next, you can add the specific paths to your ROOT_PATH with the help of the join() function. You store these two specific paths in train_data_directory and test_data_directory.\n",
    "\n",
    "\n",
    "3. You see that after, you can call the load_data() function and pass in the train_data_directory to it.\n",
    "\n",
    "\n",
    "4. Now, the load_data() function itself starts off by gathering all the subdirectories that are present in the train_data_directory; It does so with the help of list comprehension, which is quite a natural way of constructing lists - it basically says that, if you find something in the train_data_directory, you’ll double check whether this is a directory, and if it is one, you’ll add it to your list. Remember that each subdirectory represents a label.\n",
    "\n",
    "\n",
    "5. Next, you have to loop through the subdirectories. You first initialize two lists, labels and images. Next, you gather the paths of the subdirectories and the file names of the images that are stored in these subdirectories. After, you can collect the data in the two lists with the help of the append() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_directory):\n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory, d)\n",
    "        file_names = [os.path.join(label_directory, f) \n",
    "                      for f in os.listdir(label_directory) \n",
    "                      if f.endswith(\".ppm\")]\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "ROOT_PATH = \"/your/root/path\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"TrafficSigns/Training\")\n",
    "test_data_directory = os.path.join(ROOT_PATH, \"TrafficSigns/Testing\")\n",
    "\n",
    "images, labels = load_data(train_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test data are located in folders named \"Training\" and \"Testing\", which are both subdirectories of another directory \"TrafficSigns\". On a local machine, this could look something like \"/Users/Name/Downloads/TrafficSigns\", with then two subfolders called \"Training\" and \"Testing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Sign Statistics\n",
    "\n",
    "With your data loaded in, it’s time for some data inspection! You can start with a pretty simple analysis with the help of the ndim and size attributes of the images array:\n",
    "\n",
    "Note that the images and labels variables are lists, so you might need to use np.array() to convert the variables to an array in your own workspace. This has been done for you here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the `images` dimensions\n",
    "print(images.ndim)\n",
    "\n",
    "# Print the number of `images`'s elements\n",
    "print(images.size)\n",
    "\n",
    "# Print the first instance of `images`\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images[0] that you printed out is, in fact, one single image that is represented by arrays in arrays! This might seem counterintuitive at first, but it’s something that you’ll get used to as you go further into working with images in machine learning or deep learning applications.\n",
    "\n",
    "Next, you can also take a small look at the labels, but you shouldn’t see too many surprises at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the `labels` dimensions\n",
    "print(labels.ndim)\n",
    "\n",
    "# Print the number of `labels`'s elements\n",
    "print(labels.size)\n",
    "\n",
    "# Count the number of labels\n",
    "print(len(set(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers already give you some insights into how successful your import was and the exact size of your data. At first sight, everything has been executed the way you expected it to, and you see that the size of the array is considerable if you take into account that you’re dealing with arrays within arrays.\n",
    "\n",
    "**Try** adding the following attributes to your arrays to get more information about the memory layout, the length of one array element in bytes and the total consumed bytes by the array’s elements with the flags, itemsize, and nbytes attributes.\n",
    "\n",
    "Next, you can also take a look at the distribution of the traffic signs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `pyplot` module\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Make a histogram with 62 bins of the `labels` data\n",
    "plt.hist(labels, 62)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome job! Now let’s take a closer look at the histogram that you made!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You clearly see that not all types of traffic signs are equally represented in the dataset. This is something that you’ll deal with later when you’re manipulating the data before you start modeling your neural network.\n",
    "\n",
    "At first sight, you see that there are labels that are more heavily present in the dataset than others: the labels 22, 32, 38, and 61 definitely jump out. At this point, it’s nice to keep this in mind, but you’ll definitely go further into this in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing The Traffic Signs\n",
    "\n",
    "The previous, small analyses or checks have already given you some idea of the data that you’re working with, but when your data mostly consists of images, the step that you should take to explore your data is by visualizing it.\n",
    "\n",
    "Let’s check out some random traffic signs:\n",
    "\n",
    "- First, make sure that you import the pyplot module of the matplotlib package under the common alias plt.\n",
    "\n",
    "\n",
    "- Then, you’re going to make a list with 4 random numbers. These will be used to select traffic signs from the images array that you have just inspected in the previous section. In this case, you go for 300, 2250, 3650 and 4000.\n",
    "Next, you’ll say that for every element in the length of that list, so from 0 to 4, you’re going to create subplots without axes (so that they don’t go running with all the attention and your focus is solely on the images!). In these subplots, you’re going to show a specific image from the images array that is in accordance with the number at the index i. In the first loop, you’ll pass 300 to images[], in the second round 2250, and so on. Lastly, you’ll adjust the subplots so that there’s enough width in between them.\n",
    "\n",
    "\n",
    "- The last thing that remains is to show your plot with the help of the show() function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `pyplot` module of `matplotlib`\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the (random) indexes of the images that you want to see \n",
    "traffic_signs = [300, 2250, 3650, 4000]\n",
    "\n",
    "# Fill out the subplots with the random images that you defined \n",
    "for i in range(len(traffic_signs)):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[traffic_signs[i]])\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you guessed by the 62 labels that are included in this dataset, the signs are different from each other.\n",
    "\n",
    "But what else do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/thinking.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are not of the same size!\n",
    "\n",
    "You can obviously toy around with the numbers that are contained in the traffic_signs list and follow up more thoroughly on this observation, but be as it may, this is an important observation which you will need to take into account when you start working more towards manipulating your data so that you can feed it to the neural network.\n",
    "\n",
    "Let’s confirm the hypothesis of the differing sizes by printing the shape, the minimum and maximum values of the specific images that you have included into the subplots.\n",
    "\n",
    "The code below heavily resembles the one that you used to create the above plot, but differs in the fact that here, you’ll alternate sizes and images instead of plotting just the images next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `matplotlib`\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the (random) indexes of the images\n",
    "traffic_signs = [300, 2250, 3650, 4000]\n",
    "\n",
    "# Fill out the subplots with the random images and add shape, min and max values\n",
    "for i in range(len(traffic_signs)):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[traffic_signs[i]])\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n",
    "    print(\"shape: {0}, min: {1}, max: {2}\".format(images[traffic_signs[i]].shape, \n",
    "                                                  images[traffic_signs[i]].min(), \n",
    "                                                  images[traffic_signs[i]].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how you use the format() method on the string \"shape: {0}, min: {1}, max: {2}\" to fill out the arguments {0}, {1}, and {2} that you defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have seen loose images, you might also want to revisit the histogram that you printed out in the first steps of your data exploration; You can easily do this by plotting an overview of all the 62 classes and one image that belongs to each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `pyplot` module as `plt`\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Get the unique labels \n",
    "unique_labels = set(labels)\n",
    "\n",
    "# Initialize the figure\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Set a counter\n",
    "i = 1\n",
    "\n",
    "# For each unique label,\n",
    "for label in unique_labels:\n",
    "    # You pick the first image for each label\n",
    "    image = images[labels.index(label)]\n",
    "    # Define 64 subplots \n",
    "    plt.subplot(8, 8, i)\n",
    "    # Don't include axes\n",
    "    plt.axis('off')\n",
    "    # Add a title to each subplot \n",
    "    plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
    "    # Add 1 to the counter\n",
    "    i += 1\n",
    "    # And you plot this first image \n",
    "    plt.imshow(image)\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though you define 64 subplots, not all of them will show images (as there are only 62 labels!). Note also that again, you don’t include any axes to make sure that the readers’ attention doesn’t dwell far from the main topic: the traffic signs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you mostly guessed in the histogram above, there are considerably more traffic signs with labels 22, 32, 38, and 61. This hypothesis is now confirmed in this plot: you see that there are 375 instances with label 22, 316 instances with label 32, 285 instances with label 38 and, lastly, 282 instances with label 61.\n",
    "\n",
    "One of the most interesting questions that you could ask yourself now is whether there’s a connection between all of these instances - maybe all of them are designatory signs?\n",
    "\n",
    "Let’s take a closer look: you see that label 22 and 32 are prohibitory signs, but that labels 38 and 61 are designatory and a prioritory signs, respectively. This means that there’s not an immediate connection between these four, except for the fact that half of the signs that have a substantial presence in the dataset is of the prohibitory kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Now that you have a clear idea of what you need to improve, you can start with manipulating your data in such a way that it’s ready to be fed to the neural network or whichever model you want to feed it too. Let’s start first with extracting some features - you’ll rescale the images, and you’ll convert the images that are held in the images array to grayscale. You’ll do this color conversion mainly because the color matters less in classification questions like the one you’re trying to answer now. For detection, however, the color does play a big part! So in those cases, it’s not needed to do that conversion!\n",
    "\n",
    "#### Rescaling Images\n",
    "To tackle the differing image sizes, you’re going to rescale the images; You can easily do this with the help of the [skimage](https://scikit-image.org/docs/dev/api/skimage.html) or Scikit-Image library, which is a collection of algorithms for image processing.\n",
    "\n",
    "In this case, the transform module will come in handy, as it offers you a [resize() function](https://pypi.org/project/python-resize-image/); You’ll see that you make use of list comprehension (again!) to resize each image to 28 by 28 pixels. Once again, you see that the way you actually form the list: for every image that you find in the images array, you’ll perform the transformation operation that you borrow from the skimage library. Finally, you store the result in the images28 variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `transform` module from `skimage`\n",
    "from skimage import transform \n",
    "\n",
    "# Rescale the images in the `images` array\n",
    "images28 = [transform.resize(image, (28, 28)) for image in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was fairly easy wasn’t it?\n",
    "\n",
    "The images are now four-dimensional: if you convert images28 to an array and if you concatenate the attribute shape to it, you’ll see that the printout tells you that images28’s dimensions are (4575, 28, 28, 3). The images are 784-dimensional (because your images are 28 by 28 pixels).\n",
    "\n",
    "You can check the result of the rescaling operation by re-using the code that you used above to plot the 4 random images with the help of the traffic_signs variable. Just don’t forget to change all references to images to images28.\n",
    "\n",
    "Because you rescaled, your min and max values have also changed; They seem to be all in the same ranges now, which is really great because then you don’t necessarily need to normalize your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Conversion to Grayscale\n",
    "\n",
    "The color in the pictures matters less when you’re trying to answer a classification question. That’s why you’ll also go through the trouble of converting the images to grayscale.\n",
    "\n",
    "Note, however, that you can also test out on your own what would happen to the final results of your model if you don’t follow through with this specific step.\n",
    "\n",
    "Just like with the rescaling, you can again count on the Scikit-Image library to help you out; In this case, it’s the color module with its [rgb2gray() function](https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_rgb_to_gray.html) that you need to use to get where you need to be.\n",
    "\n",
    "That’s going to be nice and easy!\n",
    "\n",
    "However, don’t forget to convert the images28 variable back to an array, as the rgb2gray() function does expect an array as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `rgb2gray` from `skimage.color`\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Convert `images28` to an array\n",
    "images28 = np.array(images28)\n",
    "\n",
    "# Convert `images28` to grayscale\n",
    "images28 = rgb2gray(images28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the result of your grayscale conversion by plotting some of the images; Here, you can again re-use and slightly adapt some of the code to show the adjusted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "traffic_signs = [300, 2250, 3650, 4000]\n",
    "\n",
    "for i in range(len(traffic_signs)):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images28[traffic_signs[i]], cmap=\"gray\")\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed have to specify the color map or cmap and set it to \"gray\" to plot the images in grayscale. That is because imshow() by default uses, by default, a heatmap-like color map. Read more [here](https://stackoverflow.com/questions/39805697/skimage-why-does-rgb2gray-from-skimage-color-result-in-a-colored-image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP:** since you have been re-using this function quite a bit in this tutorial, you might look into how you can make it into a function :)\n",
    "\n",
    "These two steps are very basic ones; Other operations that you could have tried out on your data include data augmentation (rotating, blurring, shifting, changing brightness,…). If you want, you could also set up an entire pipeline of data manipulation operations through which you send your images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning With TensorFlow\n",
    "Now that you have explored and manipulated your data, it’s time to construct your neural network architecture with the help of the TensorFlow package!\n",
    "\n",
    "### Modeling The Neural Network\n",
    "Just like you might have done with Keras, it’s time to build up your neural network, layer by layer.\n",
    "\n",
    "If you haven’t done so already, import tensorflow into your workspace under the conventional alias tf. Then, you can initialize the Graph with the help of Graph(). You use this function to define the computation. Note that with the Graph, you don’t compute anything, because it doesn’t hold any values. It just defines the operations that you want to be running later.\n",
    "\n",
    "In this case, you set up a default context with the help of as_default(), which returns a context manager that makes this specific Graph the default graph. You use this method if you want to create multiple graphs in the same process: with this function, you have a global default graph to which all operations will be added if you don’t explicitly create a new graph.\n",
    "\n",
    "Next, you’re ready to add operations to your graph. As you might remember from working with Keras, you build up your model, and then in compiling it, you define a loss function, an optimizer, and a metric. This now all happens in one step when you work with TensorFlow:\n",
    "\n",
    "- First, you define placeholders for inputs and labels because you won’t put in the “real” data yet. Remember that placeholders are values that are unassigned and that will be initialized by the session when you run it. So when you finally run the session, these placeholders will get the values of your dataset that you pass in the run() function!\n",
    "\n",
    "\n",
    "- Then, you build up the network. You first start by flattening the input with the help of the flatten() function, which will give you an array of shape [None, 784] instead of the [None, 28, 28], which is the shape of your grayscale images.\n",
    "\n",
    "\n",
    "- After you have flattened the input, you construct a fully connected layer that generates logits of size [None, 62]. Logits is the function operates on the unscaled output of previous layers, and that uses the relative scale to understand the units is linear.\n",
    "\n",
    "\n",
    "- With the multi-layer perceptron built out, you can define the loss function. The choice for a loss function depends on the task that you have at hand: in this case, you make use of: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_softmax_cross_entropy_with_logits()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes sparse softmax cross entropy between logits and labels. In other words, it measures the probability error in discrete classification tasks in which the classes are mutually exclusive. This means that each entry is in exactly one class. Here, a traffic sign can only have one single label. Remember that, while regression is used to predict continuous values, classification is used to predict discrete values or classes of data points. You wrap this function with reduce_mean(), which computes the mean of elements across dimensions of a tensor.\n",
    "\n",
    "\n",
    "- You also want to define a training optimizer; Some of the most popular optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop. Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum. In this case, you pick the ADAM optimizer, for which you define the learning rate at 0.001.\n",
    "\n",
    "\n",
    "- Lastly, you initialize the operations to execute before going over to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `tensorflow` \n",
    "import tensorflow as tf \n",
    "\n",
    "# Initialize placeholders \n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 28, 28])\n",
    "y = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "\n",
    "# Flatten the input data\n",
    "images_flat = tf.contrib.layers.flatten(x)\n",
    "\n",
    "# Fully connected layer \n",
    "logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)\n",
    "\n",
    "# Define a loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, \n",
    "                                                                    logits = logits))\n",
    "# Define an optimizer \n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# Convert logits to label indexes\n",
    "correct_pred = tf.argmax(logits, 1)\n",
    "\n",
    "# Define an accuracy metric\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now successfully created your first neural network with TensorFlow!\n",
    "\n",
    "If you want, you can also print out the values of (most of) the variables to get a quick recap or checkup of what you have just coded up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"images_flat: \", images_flat)\n",
    "print(\"logits: \", logits)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted_labels: \", correct_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if you see an error like “module 'pandas' has no attribute 'computation'”, consider upgrading the packages dask by running pip install --upgrade dask in your command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Neural Network\n",
    "\n",
    "Now that you have built up your model layer by layer, it’s time to actually run it! To do this, you first need to initialize a session with the help of Session() to which you can pass your graph that you defined in the previous section. Next, you can run the session with run(), to which you pass the initialized operations in the form of the init variable that you also defined in the previous section.\n",
    "\n",
    "Next, you can use this initialized session to start epochs or training loops. In this case, you pick 201 because you want to be able to register the last loss_value; In the loop, you run the session with the training optimizer and the loss (or accuracy) metric that you defined in the previous section. You also pass a feed_dict argument, with which you feed data to the model. After every 10 epochs, you’ll get a log that gives you more insights into the loss or cost of the model.\n",
    "\n",
    "As you have seen in the section on the TensorFlow basics, there is no need to close the session manually; this is done for you. However, if you want to try out a different setup, you probably will need to do so with [sess.close()](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) if you have defined your session as sess, like in the code chunk below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(201):\n",
    "        print('EPOCH', i)\n",
    "        _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x: images28, y: labels})\n",
    "        if i % 10 == 0:\n",
    "            print(\"Loss: \", loss)\n",
    "        print('DONE WITH EPOCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you can also run the following piece of code, but that one will immediately close the session afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1234)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(201):\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict={x: images28, y: labels})\n",
    "        if i % 10 == 0:\n",
    "            print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now successfully trained your model! That wasn’t too hard, was it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Your Neural Network\n",
    "\n",
    "You’re not entirely there yet; You still need to evaluate your neural network. In this case, you can already try to get a glimpse of well your model performs by picking 10 random images and by comparing the predicted labels with the real labels.\n",
    "\n",
    "You can first print them out, but why not use matplotlib to plot the traffic signs themselves and make a visual comparison?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `matplotlib`\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Pick 10 random images\n",
    "sample_indexes = random.sample(range(len(images28)), 10)\n",
    "sample_images = [images28[i] for i in sample_indexes]\n",
    "sample_labels = [labels[i] for i in sample_indexes]\n",
    "\n",
    "# Run the \"correct_pred\" operation\n",
    "predicted = sess.run([correct_pred], feed_dict={x: sample_images})[0]\n",
    "                        \n",
    "# Print the real and predicted labels\n",
    "print(sample_labels)\n",
    "print(predicted)\n",
    "\n",
    "# Display the predictions and the ground truth visually.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(len(sample_images)):\n",
    "    truth = sample_labels[i]\n",
    "    prediction = predicted[i]\n",
    "    plt.subplot(5, 2,1+i)\n",
    "    plt.axis('off')\n",
    "    color='green' if truth == prediction else 'red'\n",
    "    plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n",
    "             fontsize=12, color=color)\n",
    "    plt.imshow(sample_images[i],  cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, only looking at random images don’t give you many insights into how well your model actually performs. That’s why you’ll load in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `skimage`\n",
    "from skimage import transform\n",
    "\n",
    "# Load the test data\n",
    "test_images, test_labels = load_data(test_data_directory)\n",
    "\n",
    "# Transform the images to 28 by 28 pixels\n",
    "test_images28 = [transform.resize(image, (28, 28)) for image in test_images]\n",
    "\n",
    "# Convert to grayscale\n",
    "from skimage.color import rgb2gray\n",
    "test_images28 = rgb2gray(np.array(test_images28))\n",
    "\n",
    "# Run predictions against the full test set.\n",
    "predicted = sess.run([correct_pred], feed_dict={x: test_images28})[0]\n",
    "\n",
    "# Calculate correct matches \n",
    "match_count = sum([int(y == y_) for y, y_ in zip(test_labels, predicted)])\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = match_count / len(test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/compat/v1/losses/sparse_softmax_cross_entropy\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
