{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep belief networks are algorithms that use probabilities and unsupervised learning to produce outputs. They are composed of binary latent variables, and they contain both undirected layers  and directed layers.\n",
    "\n",
    "Unlike other models, each layer in deep belief networks learns the entire input. In convolutional neural networks, the first layers only filter inputs for basic features, such as edges, and the later layers recombine all the simple patterns found by the previous layers. Deep belief networks, on the other hand, work globally and regulate each layer in order.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "![](assets/architecture.jpg)\n",
    "\n",
    "The network is like a stack of Restricted Boltzmann Machines (RBMs), where the nodes in each layer are connected to all the nodes in the previous and subsequent layer. However, unlike RBMs, nodes in a deep belief network do not communicate laterally within their layer. A network of symmetrical weights connect different layers.\n",
    "\n",
    "The connections in the top layers are undirected and associative memory is formed from the connections between them. The connections in the lower levels are directed.\n",
    "\n",
    "The nodes in the hidden layer fulfill two roles━they act as a hidden layer to nodes that precede it and as visible layers to nodes that succeed it. These nodes identify the correlations in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do they work?\n",
    "\n",
    "Greedy learning algorithms are used to pre-train deep belief networks. This is a problem-solving approach that involves making the optimal choice at each layer in the sequence, eventually finding a global optimum.\n",
    "\n",
    "Greedy learning algorithms start from the bottom layer and move up, fine-tuning the generative weights. The learning takes place on a layer-by-layer basis, meaning the layers of the deep belief networks are trained one at a time. Therefore, each layer also receives a different version of the data, and each layer uses the output from the previous layer as their input.\n",
    "\n",
    "Greedy learning algorithms are used to train deep belief networks because they are quick and efficient. Moreover, they help to optimize the weights at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications Of Deep Belief Networks\n",
    "\n",
    "\n",
    "\n",
    "**Image recognition**\n",
    "Deep belief networks can be used in image recognition. A picture would be the input, and the category the output. This technology has broad applications, ranging from relatively simple tasks like photo organization to critical functions like medical diagnoses. For example, smart microspores that can perform image recognition could be used to classify pathogens. This would alleviate the reliance on rare specialists during serious epidemics, reducing the response time.\n",
    "\n",
    "**Video recognition**\n",
    "Video recognition also uses deep belief networks. Video recognition works similarly to vision, in that it finds meaning in the video data. For example, it can identify an object or a gesture of a person. It can be used in many different fields such as home automation, security and healthcare.\n",
    "\n",
    "**Motion-capture data**\n",
    "Motion capture data involves tracking the movement of objects or people and also uses deep belief networks. Motion capture is tricky because a machine can quickly lose track of, for example, a person━if another person that looks similar enters the frame or if something obstructs their view temporarily. Motion capture thus relies not only on what an object or person look like but also on velocity and distance. Motion capture is widely used in video game development and in filmmaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing a Deep Belief Network in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages\n",
    "\n",
    "We’ll start by importing the packages that we’ll need. We’ll import train_test_split  (to generate our training and testing splits of the MNIST dataset) and classification_report  (to display a nicely formatted table of accuracies) from the scikit-learn package. We’ll import the dataset  module from scikit-learn to download the MNIST dataset.\n",
    "\n",
    "Next up, we’ll import our Deep Belief Network implementation from the nolearn  package.\n",
    "\n",
    "And finally we’ll wrap up our import  statements by importing NumPy for numerical processing and cv2  for our OpenCV bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from nolearn.dbn import DBN\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go ahead and download the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.fetch_openml(\"mnist_784\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual dataset is roughly 55mb so it may take a few seconds to download. However, once the dataset is downloaded it is cached locally on your machine so you will not have to download it again.\n",
    "\n",
    "If you take the time to examine the data, you’ll notice that each feature vector contains 784 entries in the range [0, 255]. These values are the grayscale pixel intensities of the flattened 28 x 28 image. Background pixels are black (0) whereas foreground pixels appear to be lighter shades of gray or white.\n",
    "\n",
    "Time to generate our training and testing splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data to the range [0, 1] and then construct the training\n",
    "(trainX, testX, trainY, testY) = train_test_split(dataset.data / 255.0, dataset.target.astype(\"int0\"), test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our Deep Belief network, we’ll need two sets of data — a set for training our algorithm and a set for evaluating or testing the performance of the classifier.\n",
    "\n",
    "The first argument we specify is the data itself, which we scale to be in range [0, 1.0]. The Deep Belief Network assumes that our data is scaled in the range [0, 1.0] so this is a necessary step.\n",
    "\n",
    "We then specify the “target” or the “class labels” for each feature vector as the second argument.\n",
    "\n",
    "The last argument to train_test_split  is the size of our testing set. We’ll utilize 33% of the data for testing, while the remaining 67% will be utilized for training our Deep Belief Network.\n",
    "\n",
    "Speaking of training the Deep Belief Network, let’s go ahead and do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Deep Belief Network with 784 input units (the flattened,\n",
    "# 28x28 grayscale image), 300 hidden units, 10 output units (one for\n",
    "# each possible output classification, which are the digits 1-10)\n",
    "dbn = DBN(\n",
    "    [trainX.shape[1], 300, 10],\n",
    "    learn_rates = 0.3,\n",
    "    learn_rate_decays = 0.9,\n",
    "    epochs = 10,\n",
    "    verbose = 1)\n",
    "dbn.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument details the structure of our network, represented as a list. The first entry in the list is the number of nodes in our input layer. We’ll want to have an input node for each entry in our feature vector list, so we’ll specify the length of the feature vector for this value.\n",
    "\n",
    "Our input layer will now feed forward into our second entry in the list, a hidden layer. This hidden layer will be represented as RBM with 300 nodes.\n",
    "\n",
    "Finally, the output of the 300 node hidden layer will be fed into the output layer, which consists of an output for each of the class labels.\n",
    "\n",
    "We can then define our learn_rate , which is the learning rate of the algorithm, the decay of the learn rate (learn_rate_decays ), the number of epochs , or iterations of the training data, and the verbosity level.\n",
    "\n",
    "Both learn_rates  and learn_rates_decays  can be specified as a single floating point values or a list of floating point values. If you specify only a single value, this learning rate/decay rate will be applied to all layers in the network. If you specify a list of values, the the corresponding learning rate and decay rate will be used for the respective layers.\n",
    "\n",
    "Training the actual algorithm takes place. If you have a slow machine, you way want to make a cup of coffee or go for a quick walk during this time.\n",
    "\n",
    "Now that our Deep Belief Network is trained, let’s go ahead and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions for the test data and show a classification\n",
    "# report\n",
    "preds = dbn.predict(testX)\n",
    "print classification_report(testY, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a call to the predict method of the network which takes our testing data and makes predictions regarding which digit each image contains. If you have worked with scikit-learn at all, then this should feel very natural and comfortable.\n",
    "\n",
    "We then present a table of accuracies.\n",
    "\n",
    "Finally, it might be interesting to inspect images individually rather than on aggregate as a further demonstration of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a few of the test instances\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size = (10,)):\n",
    "    # classify the digit\n",
    "    pred = dbn.predict(np.atleast_2d(testX[i]))\n",
    "    # reshape the feature vector to be a 28x28 pixel image, then change\n",
    "    # the data type to be an unsigned 8-bit integer\n",
    "    image = (testX[i] * 255).reshape((28, 28)).astype(\"uint8\")\n",
    "    # show the image and prediction\n",
    "    print \"Actual digit is {0}, predicted {1}\".format(testY[i], pred[0])\n",
    "    cv2.imshow(\"Digit\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Fire up a shell, navigate to your dbn.py  file, and issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python dbn.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that our Deep Belief Network is trained over 10 epochs (iterations over the training data). At each iteration our our loss function is minimized and the error on the training set is lower.\n",
    "\n",
    "Taking a look at our classification report we see that we have obtained pretty high accuracy (the precision column) on our testing set. As you can see, the “1” and “7” digits was accurately classified 99% of the time. We could have perhaps obtained higher accuracy for the other digits had we let our network train for more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras: \n",
    "\n",
    "Try one by one all the digits to see if they have benn classified correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links:\n",
    "- https://deepai.org/machine-learning-glossary-and-terms/deep-belief-network\n",
    "- https://www.techopedia.com/definition/33289/deep-belief-network-dbn\n",
    "- http://primo.ai/index.php?title=Deep_Belief_Network_(DBN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
