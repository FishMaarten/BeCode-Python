{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will let you deploy in production a Machine Learning model starting from zero. Here are the steps you’re going to cover:\n",
    "\n",
    " 1. Define your goal\n",
    " 2. Load data\n",
    " 3. Data exploration\n",
    " 4. Data preparation\n",
    " 5. Build and evalute your model\n",
    " 6. Save the model\n",
    " 7. Build REST API\n",
    " 8. Deploy to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objective/goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, you need to know why you need a Machine Learning (ML) model in the first place. Knowing the objective gives you insights about:\n",
    "\n",
    "Is ML the right approach?\n",
    "What data do I need?\n",
    "What a “good model” will look like? What metrics can I use?\n",
    "How do I solve the problem right now? How accurate is the solution?\n",
    "How much is it going to cost to keep this model running?\n",
    "In our example, we’re trying to predict Airbnb listing price per night in NYC. Our objective is clear - given some data, we want our model to predict how much will it cost to rent a certain property per night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "The data comes from Airbnb Open Data and it is hosted on [Kaggle](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 16, 10\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from Google Drive with gdown ( in terminal):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1aRXGcJlIkuC6uj1iLqzi9DQQS-3GPwM_ --output airbnb_nyc.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load it into a Pandas DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('airbnb_nyc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "This step is crucial. The goal is to get a better understanding of the data. You might be tempted to jumpstart the modeling process, but that would be suboptimal. Looking at large amounts of examples, looking for patterns and visualizing distributions will build your intuition about the data. That intuition will be helpful when modeling, imputing missing data and looking at outliers.\n",
    "\n",
    "One easy way to start is to count the number of rows and columns in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 48,895 rows and 16 columns. Enough data to do something interesting.\n",
    "\n",
    "Let’s start with the variable we’re trying to predict price. To plot the distribution, we’ll use distplot():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a highly skewed distribution with some values in the 10,000 range (you might want to explore those). We’ll use a trick - log transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log1p(df.price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more like a normal distribution. Turns out this might help your model better learn the data. You’ll have to remember to preprocess the data before training and predicting.\n",
    "\n",
    "The type of room seems like another interesting point. Let’s have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='room_type', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most listings are offering entire places or private rooms. What about the location? What neighborhood groups are most represented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='neighbourhood_group', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Manhattan leads the way. Obviously, Brooklyn is very well represented, too. You can thank Mos Def, Nas, Masta Ace, and Fabolous for that.\n",
    "\n",
    "Another interesting feature is the number of reviews. Let’s have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.number_of_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one seems to follow a Power law (it has a fat tail). This one seems to follow a Power law (it has a fat tail). There seem to be some outliers (on the right) that might be of interest for investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Correlations\n",
    "\n",
    "The correlation analysis might give you hints at what features might have predictive power when training your model.\n",
    "\n",
    "Computing Pearson correlation coefficient between a pair of features is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the correlation of the price with the other attributes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_corr = corr_matrix['price']\n",
    "price_corr.iloc[price_corr.abs().argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient is defined in the -1 to 1 range. A value close to 0 means there is no correlation. Value of 1 suggests a perfect positive correlation (e.g. as the price of Bitcoin increases, your dreams of owning more are going up, too!). Value of -1 suggests perfect negative correlation (e.g. high number of bad reviews should correlate with lower prices).\n",
    "\n",
    "The correlation in our dataset looks really bad. Luckily, categorical features are not included here. They might have some predictive power too! How can we use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "The goal here is to transform the data into a form that is suitable for your model. There are several things you want to do when handling (think CSV, Database) structured data:\n",
    "\n",
    "- Handle missing data\n",
    "- Remove unnecessary columns\n",
    "- Transform any categorical features to numbers/vectors\n",
    "- Scale numerical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "Let’s start with a check for missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing[missing > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll just go ahead and remove those features for this example. In real-world applications, you should consider other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\n",
    "    'id', 'name', 'host_id', 'host_name',\n",
    "    'reviews_per_month', 'last_review', 'neighbourhood'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re also dropping the neighbourhood, host id (too many unique values), and the id of the listing.\n",
    "\n",
    "Next, we’re splitting the data into features we’re going to use for the prediction and a target variable y (the price):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = np.log1p(df.price.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We’re applying the log transformation to the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling and categorical data\n",
    "\n",
    "Let’s start with feature scaling. Specifically, we’ll do min-max normalization and scale the features in the 0-1 range. Luckily, the [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from scikit-learn does just that.\n",
    "\n",
    "But why do feature scaling at all? Largely because of the algorithm we’re going to use to train our model will do better with it.\n",
    "\n",
    "Next, we need to preprocess the categorical data. Why?\n",
    "\n",
    "Some Machine Learning algorithms can operate on categorical data without any preprocessing (like Decision trees, Naive Bayes). But most can’t.\n",
    "\n",
    "Unfortunately, you can’t replace the category names with a number. Converting Brooklyn to 1 and Manhattan to 2 suggests that Manhattan is greater (2 times) than Brooklyn. That doesn’t make sense. How can we solve this?\n",
    "\n",
    "We can use One-hot encoding. To get a feel of what it does, we’ll use OneHotEncoder from scikit-learn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data = [['Manhattan'], ['Brooklyn']]\n",
    "OneHotEncoder(sparse=False).fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, you get a vector for each value that contains 1 at the index of the category and 0 for every other value. This encoding solves the comparison issue. The negative part is that your data now might take much more memory.\n",
    "\n",
    "All data preprocessing steps are to be performed on the training data and data we’re going to receive via the REST API for prediction. We can unite the steps using [make_column_transformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "transformer = make_column_transformer(\n",
    "    (MinMaxScaler(), [\n",
    "      'latitude', 'longitude', 'minimum_nights',\n",
    "      'number_of_reviews', 'calculated_host_listings_count', 'availability_365'\n",
    "    ]),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\n",
    "      'neighbourhood_group', 'room_type'\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enumerate all columns that need feature scaling and one-hot encoding. Those columns will be replaced with the ones from the preprocessing steps. Next, we’ll learn the ranges and categorical mapping using our transformer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll transform our data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing is to separate the data into training and test sets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re going to use only the training set while developing and evaluating your model. The test set will be used later.\n",
    "\n",
    "That’s it! You are now ready to build a model !  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your model\n",
    "\n",
    "\n",
    "Finally, it is time to do some modeling. Recall the goal we set for ourselves at the beginning: \"We’re trying to predict Airbnb listing price per night in NYC\"\n",
    "\n",
    "We have a price prediction problem on our hands. More generally, we’re trying to predict a numerical value defined in a very large range. This fits nicely in the Regression Analysis framework.\n",
    "\n",
    "Training a model boils down to minimizing some predefined error. What error should we measure?\n",
    "\n",
    "## Error measurement\n",
    "We’ll use [Mean Squared Error](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/) which measures the difference between average squared predicted and true values:\n",
    "\n",
    "![](assets/mse.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Deep Neural Network with Keras\n",
    "\n",
    "Keras is the official high-level API for TensorFlow. In short, it allows you to build complex models using a sweet interface. Let’s build a model with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(\n",
    "  units=64,\n",
    "  activation=\"relu\",\n",
    "  input_shape=[X_train.shape[1]]\n",
    "))\n",
    "model.add(keras.layers.Dropout(rate=0.3))\n",
    "model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential API allows you to add various layers to your model, easily. Note that we specify the input_size in the first layer using the training data. We also do regularization using [Dropout layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).\n",
    "\n",
    "How can we specify the error metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.0001),\n",
    "    loss = 'mae',\n",
    "    metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compile() method lets you specify the optimizer and the error metric you need to reduce.\n",
    "\n",
    "Your model is ready for training. Let’s go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training a Keras model involves calling a single method - fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "  monitor='val_mae',\n",
    "  mode=\"min\",\n",
    "  patience=10\n",
    ")\n",
    "history = model.fit(\n",
    "  x=X_train,\n",
    "  y=y_train,\n",
    "  shuffle=True,\n",
    "  epochs=100,\n",
    "  validation_split=0.2,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the training method with the training data and specify the following parameters:\n",
    "\n",
    "- shuffle - random sort the data\n",
    "- epochs - number of training cycles\n",
    "- validation_split - use some percent of the data for measuring the error and not during training\n",
    "- batch_size - the number of training examples that are fed at a time to our model\n",
    "- callbacks - we use [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to prevent our model from overfitting when the training and validation error start to diverge\n",
    "\n",
    "\n",
    "After the long training process is complete, you need to answer one question. Can your model make good predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "One simple way to understand the training process is to look at the training and validation loss:\n",
    "\n",
    "![](assets/ex1.jpg)\n",
    "\n",
    "We can see a large improvement in the training error, but not much on the validation error. What else can we use to test our model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the test data\n",
    "\n",
    "Recall that we have some additional data. Now it is time to use it and test how good our model. Note that we don’t use that data during the training, only once at the end of the process.\n",
    "\n",
    "Let’s get the predictions from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we’ll use a couple of metrics for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "print(f'MSE {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'RMSE {np.sqrt(mean_squared_error(y_test, y_pred))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve already discussed MSE. You can probably guess what Root Mean Squared Error (RMSE) means. RMSE allows us to penalize points further from the mean.\n",
    "\n",
    "Another statistic we can use to measure how well our predictions fit with the real data is the R^2R \n",
    "2\n",
    "  score. A value close to 1 indicates a perfect fit. Let’s check ours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definitely room for improvement here. You might try to tune the model better and get better results.\n",
    "\n",
    "Now you have a model and a rough idea of how well will it do in production. How can you save your work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "Now that you have a trained model, you need to store it and be able to reuse it later. Recall that we have a data transformer that needs to be stored, too! Let’s save both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(transformer, \"data_transformer.joblib\")\n",
    "model.save(\"price_prediction_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended approach of storing scikit-learn models is to use [joblib](https://joblib.readthedocs.io/en/latest/). Saving the model architecture and weights of a Keras model is done with the save() method.\n",
    "\n",
    "You can download the files from the notebook using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"data_transformer.joblib\")\n",
    "files.download(\"price_prediction_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build REST API\n",
    "\n",
    "Building a REST API allows you to use your model to make predictions for different clients. Almost any device can speak REST - Android, iOS, Web browsers, and many others.\n",
    "\n",
    "[Flask](https://www.fullstackpython.com/flask.html) allows you to build a REST API in just a couple of lines. Of course, we’re talking about a quick-and-dirty prototype. Let’s have a look at the complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import expm1\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from flask import Flask, jsonify, request\n",
    "from tensorflow import keras\n",
    "app = Flask(__name__)\n",
    "model = keras.models.load_model(\"assets/price_prediction_model.h5\")\n",
    "transformer = joblib.load(\"assets/data_transformer.joblib\")\n",
    "@app.route(\"/\", methods=[\"POST\"])\n",
    "def index():\n",
    "    data = request.json\n",
    "    df = pd.DataFrame(data, index=[0])\n",
    "    prediction = model.predict(transformer.transform(df))\n",
    "    predicted_price = expm1(prediction.flatten()[0])\n",
    "    return jsonify({\"price\": str(predicted_price)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete project (including the data transformer and model) is on [GitHub](https://github.com/curiousily/Deploy-Keras-Deep-Learning-Model-with-Flask).\n",
    "\n",
    "The API has a single route (index) that accepts only POST requests. Note that we pre-load the data transformer and the model.\n",
    "\n",
    "The request handler obtains the JSON data and converts it into a Pandas DataFrame. Next, we use the transformer to pre-process the data and get a prediction from our model. We invert the log operation we did in the pre-processing step and return the predicted price as JSON.\n",
    "\n",
    "Your REST API is ready to go. Run the following command in the project directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it in the shell\n",
    "flask run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new tab to test the API:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -d '{\"neighbourhood_group\": \"Brooklyn\", \"latitude\": 40.64749, \"longitude\": -73.97237, \"room_type\": \"Private room\", \"minimum_nights\": 1, \"number_of_reviews\": 9, \"calculated_host_listings_count\": 6, \"availability_365\": 365}' -H \"Content-Type: application/json\" -X POST http://localhost:5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"price\":\"72.70381414559431\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to production\n",
    "\n",
    "We’ll deploy the project to [Google App Engine](https://cloud.google.com/appengine/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "App Engine allows us to use Python and easily deploy a Flask app.\n",
    "\n",
    "You need to:\n",
    "\n",
    "- Register for Google Cloud Engine account\n",
    "- Google Cloud SDK installed\n",
    "\n",
    "\n",
    "Here is the complete **app.yaml** config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoint: \"gunicorn -b :$PORT app:app --timeout 500\"\n",
    "runtime: python\n",
    "env: flex\n",
    "service: nyc-price-prediction\n",
    "runtime_config:\n",
    "  python_version: 3.7\n",
    "instance_class: B1\n",
    "manual_scaling:\n",
    "  instances: 1\n",
    "liveness_check:\n",
    "  path: \"/liveness_check\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following command to deploy the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it in terminal\n",
    "gcloud app deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the process to complete and test the API running on production. You did it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links:\n",
    "- https://arxiv.org/abs/1502.03167\n",
    "- https://www.adesso.ch/de_ch/news/blog/deploy-keras-models-to-production-level-easily.jsp\n",
    "- https://github.com/tushar-31093/How-to-Deploy-Keras-Models-to-Production\n",
    "- https://datascience.stackexchange.com/questions/45086/production-tensorflow-and-keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
